---
title: "x-tagger: A Natural Language Processing Toolkit for Token Classification In Its Simplest Form"
date: "2021-04-20"
tages: [Python Library, Part-of-speech Tagging, Computational Linguistics]
header:
  overlay_image: "/images/xtagger/CCCCFF.jpeg"
  teaser: "/images/xtagger/logo.png"
TeX: {
  extensions: ["AMSmath.js", "AMSsymbols.js"]
}
excerpt: "Python Library, Part-of-speech Tagging, Computational Linguistics"
mathjax: "true"
image:
  thumb: "/images/xtagger/logo.png"
---

{: .text-justify}
x-tagger is a Natural Language Processing library for token classification (POS Tagging etc.). The reason why I called "in its simplest form" is the highest abstraction of it. You can train models and make inference in 5-10 lines of code. Other powerful feature of x-tagger is that it support most common dataset types. What does it mean? For example, [torchtext](https://pytorch.org/text/stable/index.html) is a common library for [PyTorch](https://pytorch.org/) for Natural Language Processing. Or, it is easy to train [huggingface transformers](https://huggingface.co/transformers/) models with [huggingface datasets](https://huggingface.co/docs/datasets/#).

{: .text-justify}
x-tagger packs all of these powerful features together. To train a x-tagger model, you need a **most simplest form** of a POS tagging dataset. In this post, I call it as "x-tagger dataset" but it is nothing but list of tuple lists:

```
[
[('It', 'PRON'), ('was', 'VERB'), ('outrageous', 'ADJ'), ('.', '.')],

[('``', '.'), ('Both', 'DET'), ('sides', 'NOUN'), ('are', 'VERB'), 
 ('taking', 'VERB'), ('action', 'NOUN'), ('.', '.'), ("''", '.')],
...
]
```

{: .text-justify}
x-tagger is currently in beta release. It supports only Hidden Markov Model, Long Short-Term Memory and BERT.

## Tagging With Hidden Markov Model

{: .text-justify}
Hidden Markov Model (HMM) is a statistical Markov model with Markov assumption:

$$P(q_i = \beta \mid q_1 q_2 ...q_{i-1}) \approx p(q_i = \beta \mid q_{i-1})$$

{: .text-justify}
Hidden Markov Model for tagging allows us to talk about both observed words (events) and part-of-speech tags (hidden events) that we think of as causal factors in our probabilistic model. Formally defined:

$$
\begin{align}
 Q &= q_1 q_2 q_3 ... q_n \;\;\; &\text{states}\\
 A &= a_{11}...a_{ij}...a_{nn} \;\;\; &\text{transition probability matrix}\\
 O &= o_1 o_2...o_T \;\;\; &\text{a sequence of T observations}\\
 B &= b_i(o_t) \;\;\; &\text{emission probabilities}\\
 \pi &= \pi_1,..., \pi_n \;\;\; &\text{initial distribution over states}\\
\end{align}
$$

{: .text-justify}
For token classification, **transition probability** means "probability of tag $$t_i$$ with observing $$t_{i-1}$$":

{: .text-justify}
$$p(t_{i} \mid t_{i-1}) = \frac{\text{count}(t_{i-1}, t_{i})}{\text{count}(t_{i-1})}$$

{: .text-justify}
and **emission probability** means "probability of word $$w_i$$ with observing its tag $$t_i$$":

$$p(w_{i} \mid t_{i}) = \frac{\text{count}(t_{i}, w_{i})}{\text{count}(t_{i})}$$

{: .text-justify}
The "training procedure" of HMM is calculating emission and transition probabilities. This probabilities is obtained from tagged training dataset. The formulations of these probabilites is based on bigram Markov assumption. x-tagger has bigram and trigram options.

{: .text-justify}
Tagging unobserved samples or evaluation is done by **Viterbi Decoding** with dynamical programming in x-tagger.

The decoding is to choose the tag sequence $$\mathbf{t}$$ that is most probable given the observation sequence of $$n$$ words $$\mathbf{w}$$:

$$ \hat{\mathbf{t}} = \underset{\theta}{\operatorname{argmax}} p(\mathbf{t} \mid \mathbf{w}) $$

using Bayes' rule, we have:

$$
\begin{align}
\hat{\mathbf{t}} &= \underset{\theta}{\operatorname{argmax}} \frac{p(\mathbf{w} \mid \mathbf{t}) \cdot p(\mathbf{t})}{p(\mathbf{w})}\\
&= \underset{\theta}{\operatorname{argmax}} p(\mathbf{w} \mid \mathbf{t}) \cdot p(\mathbf{t})
\end{align}
$$

The probabilities are obtained from bigram assumption that was mentioned above

$$
\begin{align}
p(\mathbf{w} \mid \mathbf{t}) &\approx \prod_{i=1}^n p(w_i \mid t_i) \\
p(\mathbf{t}) &\approx \prod_{i=1}^n p(t_{i} \mid t_{i-1})
\end{align}
$$

plugging those equations, we have

$$\mathbf{t} = \underset{\theta}{\operatorname{argmax}} p(\mathbf{w} \mid \mathbf{t}) \cdot p(\mathbf{t}) = \prod_{i=1}^n p(w_i \mid t_i) \cdot p(t_{i} \mid t_{i-1})$$
