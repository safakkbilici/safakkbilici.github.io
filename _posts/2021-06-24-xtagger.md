---
title: "x-tagger: A Natural Language Processing Toolkit for Token Classification In Its Simplest Form"
date: "2021-04-20"
tages: [Python Library, Part-of-speech Tagging, Computational Linguistics]
header:
  overlay_image: "/images/xtagger/CCCCFF.jpeg"
  teaser: "/images/xtagger/logo.png"
TeX: {
  extensions: ["AMSmath.js", "AMSsymbols.js"]
}
excerpt: "Python Library, Part-of-speech Tagging, Computational Linguistics"
mathjax: "true"
image:
  thumb: "/images/xtagger/logo.png"
---

{: .text-justify}
x-tagger is a Natural Language Processing library for token classification (POS Tagging etc.). The reason why I called "in its simplest form" is the highest abstraction of it. You can train models and make inference in 5-10 lines of code. Other powerful feature of x-tagger is that it support most common dataset types. What does it mean? For example, [torchtext](https://pytorch.org/text/stable/index.html) is a common library for [PyTorch](https://pytorch.org/) for Natural Language Processing. Or, it is easy to train [huggingface transformers](https://huggingface.co/transformers/) models with [huggingface datasets](https://huggingface.co/docs/datasets/#).

x-tagger packs all of these powerful features together. To train a x-tagger model, you need a **most simplest form** of a POS tagging dataset. In this post, I call it as "x-tagger dataset" but it is nothing but list of tuple lists:

```
[
[('It', 'PRON'), ('was', 'VERB'), ('outrageous', 'ADJ'), ('.', '.')],

[('``', '.'), ('Both', 'DET'), ('sides', 'NOUN'), ('are', 'VERB'), 
 ('taking', 'VERB'), ('action', 'NOUN'), ('.', '.'), ("''", '.')],
...
]
```

x-tagger is currently in beta release. It supports only Hidden Markov Model, Long Short-Term Memory and BERT.

## Tagging With Hidden Markov Model

Hidden Markov Model (HMM) is a statistical Markov model with Markov assumption:

$$P(q_i = \beta \mid q_1 q_2 ...q_{i-1}) \approx p(q_i = \beta \mid q_{i-1})$$

Hidden Markov Model for tagging allows us to talk about both observed words (events) and part-of-speech tags (hidden events) that we think of as causal factors in our probabilistic model. Formally defined:

$$
\begin{align}
 Q &= q_1 q_2 q_3 ... q_n \;\;\; \text{states}\\
 A &= a_{11}...a_{ij}...a_{nn} \;\;\; \text{transition probability matrix}\\
 O &= o_1 o_2...o_T \;\;\; \text{a sequence of T observations}\\
 B &= b_i(o_t) \;\;\; \text{emission probabilities}\\
 \pi &= \pi_1,..., \pi_n \;\;\; \text{initial probability distribution over states}\\
\end{align}
$$
